{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":128431,"databundleVersionId":15477148,"sourceType":"competition"}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Milestone 1\n\nThis milestone focuses on understanding the dataset and establishing a baseline performance through **exploratory data analysis (EDA)** and simple **heuristic-based methods** using `librosa`.\n\n---\n\n## Suggested Readings\n- [Hugging Face Audio Course](https://huggingface.co/learn/audio-course/en/chapter0/introduction)\n- [Librosa Documentation](https://librosa.org/doc/main/core.html#audio-loading)\n\n---\n\n## Instructions\nUse this notebook to answer **all Milestone-1 questions**.\n\n---\n\n## Resources\n- Notebook Link:  \n  https://colab.research.google.com/drive/1m6UczhxQIke_raWSqukSWuiKbIVt7MMb?usp=sharing  \n\n- Competition Link:  \n  https://www.kaggle.com/competitions/jan-2026-dl-gen-ai-project/\n","metadata":{"id":"2OhS0NNSgfjf"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport random\nimport torch\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"P98avCy_pVzP","trusted":true,"execution":{"iopub.status.busy":"2026-02-18T16:36:27.240775Z","iopub.execute_input":"2026-02-18T16:36:27.241121Z","iopub.status.idle":"2026-02-18T16:36:27.246636Z","shell.execute_reply.started":"2026-02-18T16:36:27.241092Z","shell.execute_reply":"2026-02-18T16:36:27.245567Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"#----------------------------- DON'T CHANGE THIS --------------------------\nDATA_SEED = 67\nTRAINING_SEED = 1234\nSR = 22050\nDURATION = 5.0\nN_FFT = 2048\nHOP_LENGTH = 512\nN_MELS = 128\nTOP_DB=20\nTARGET_SNR_DB = 10\n\nrandom.seed(DATA_SEED)\nnp.random.seed(DATA_SEED)\ntorch.manual_seed(DATA_SEED)\ntorch.cuda.manual_seed(DATA_SEED)","metadata":{"id":"FJDKs-0cpVzR","trusted":true,"execution":{"iopub.status.busy":"2026-02-18T16:36:27.248326Z","iopub.execute_input":"2026-02-18T16:36:27.248735Z","iopub.status.idle":"2026-02-18T16:36:27.264351Z","shell.execute_reply.started":"2026-02-18T16:36:27.248700Z","shell.execute_reply":"2026-02-18T16:36:27.263461Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# CONFIGURATION\nDATA_ROOT = \"/kaggle/input/jan-2026-dl-gen-ai-project/messy_mashup/genres_stems/\"\nGENRES = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\nSTEMS = ['bass.wav', 'drums.wav', 'other.wav', 'vocals.wav']\nSTEM_KEYS = ['drums', 'vocals', 'bass', 'other']\nGENRE_TO_TEST = 'rock'\nSONG_INDEX = 0","metadata":{"id":"3bcdenPtpVzS","trusted":true,"execution":{"iopub.status.busy":"2026-02-18T16:36:27.265477Z","iopub.execute_input":"2026-02-18T16:36:27.265800Z","iopub.status.idle":"2026-02-18T16:36:27.282748Z","shell.execute_reply.started":"2026-02-18T16:36:27.265770Z","shell.execute_reply":"2026-02-18T16:36:27.281799Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def build_dataset(root_dir, val_split=0.17, seed=42):\n    # Initialize empty dictionaries\n    train_dataset = {g: {s.replace('.wav', ''): [] for s in STEMS} for g in GENRES}\n    val_dataset   = {g: {s.replace('.wav', ''): [] for s in STEMS} for g in GENRES}\n\n    rng = random.Random(seed)\n\n    # ------------------- write your code here -------------------------------\n    \n    # Iterate through Genres\n    for genre in tqdm(GENRES, desc=\"Processing Genres\"):\n        genre_path = os.path.join(root_dir, genre)\n        \n        # Check: if genre folder exists\n        if not os.path.exists(genre_path):\n            print(f\"Warning: Genre folder {genre} not found!\")\n            continue\n        \n        # Get all song folders\n        song_folders = sorted([f for f in os.listdir(genre_path) if os.path.isdir(os.path.join(genre_path, f))])\n        \n        # Filter valid songs\n        valid_songs = []\n        for song_folder in song_folders:\n            song_path = os.path.join(genre_path, song_folder)\n            \n            # CHECK : Completeness (Does it have all stems?)\n            stems_present = [os.path.join(song_path, stem) for stem in STEMS]\n            if not all(os.path.exists(stem_path) for stem_path in stems_present):\n                continue  # Skip incomplete songs\n            \n            # CHECK : Corruption (Is any file too small? (less than 4kb))\n            # size checks\n            corrupted = False\n            for stem_path in stems_present:\n                file_size = os.path.getsize(stem_path)\n                if file_size < 4 * 1024:  # 4KB = 4 * 1024 bytes\n                    corrupted = True\n                    break\n            \n            if not corrupted:\n                valid_songs.append(song_folder)\n        \n        # Stratified Shuffle Split\n        rng.shuffle(valid_songs)\n        split_idx = int(len(valid_songs) * (1 - val_split))\n        train_songs = valid_songs[:split_idx]\n        val_songs = valid_songs[split_idx:]\n        \n        # Helper function to populate dict\n        def add_to_dict(target_dict, song_list):\n            for song_folder in song_list:\n                song_path = os.path.join(genre_path, song_folder)\n                for stem in STEMS:\n                    stem_name = stem.replace('.wav', '')\n                    stem_path = os.path.join(song_path, stem)\n                    target_dict[genre][stem_name].append(stem_path)\n        \n        add_to_dict(train_dataset, train_songs)\n        add_to_dict(val_dataset, val_songs)\n     #-------------------------------------------------------------------------\n\n    return train_dataset, val_dataset\n\ntr, val = build_dataset(DATA_ROOT)","metadata":{"id":"SD382v9NpVzS","trusted":true,"execution":{"iopub.status.busy":"2026-02-18T16:36:27.284056Z","iopub.execute_input":"2026-02-18T16:36:27.284402Z","iopub.status.idle":"2026-02-18T16:36:32.119928Z","shell.execute_reply.started":"2026-02-18T16:36:27.284368Z","shell.execute_reply":"2026-02-18T16:36:32.119188Z"}},"outputs":[{"name":"stderr","text":"Processing Genres: 100%|██████████| 10/10 [00:04<00:00,  2.08it/s]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ---- Q1-Q3 Analysis ----\nMB = 1024 * 1024\nthreshold_small = 5.0491 * MB   # < 5.0491 MB\nthreshold_large = 5.0493 * MB   # > 5.0493 MB\n\ncorrupted_count = 0\nsmall_files     = 0\nlarge_files     = 0\n\nfor genre in GENRES:\n    genre_path = os.path.join(DATA_ROOT, genre)\n    if not os.path.exists(genre_path):\n        continue\n    for song_folder in os.listdir(genre_path):\n        song_path = os.path.join(genre_path, song_folder)\n        if not os.path.isdir(song_path):\n            continue\n        for stem in STEMS:\n            stem_path = os.path.join(song_path, stem)\n            if os.path.exists(stem_path):\n                file_size = os.path.getsize(stem_path)\n                if file_size < 4 * 1024:\n                    corrupted_count += 1\n                if file_size < threshold_small:\n                    small_files += 1\n                if file_size > threshold_large:\n                    large_files += 1\n\nprint(f\"Corrupted songs (< 4KB)  : {corrupted_count}\")\nprint(f\"Total songs < 5.0491MB   : {small_files}\")\nprint(f\"Total songs > 5.0493MB   : {large_files}\")\nprint()\n\n# Q1\nq1 = corrupted_count + small_files\nprint(f\"[Q1] Corrupted + Small   = {corrupted_count} + {small_files} = {q1}\")\n\n# Q2\nq2 = abs(large_files - small_files)\nprint(f\"[Q2] |Large - Small|     = |{large_files} - {small_files}| = {q2}\")\n\n# Q3\ntrain_reggae_drums  = len(tr['reggae']['drums'])\nval_country_vocals  = len(val['country']['vocals'])\nq3 = abs(train_reggae_drums - val_country_vocals)\nprint(f\"[Q3] |Train reggae drums - Val country vocals| = |{train_reggae_drums} - {val_country_vocals}| = {q3}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T16:36:32.121574Z","iopub.execute_input":"2026-02-18T16:36:32.121818Z","iopub.status.idle":"2026-02-18T16:36:33.341076Z","shell.execute_reply.started":"2026-02-18T16:36:32.121797Z","shell.execute_reply":"2026-02-18T16:36:33.340280Z"}},"outputs":[{"name":"stdout","text":"Corrupted songs (< 4KB)  : 0\nTotal songs < 5.0491MB   : 1256\nTotal songs > 5.0493MB   : 184\n\n[Q1] Corrupted + Small   = 0 + 1256 = 1256\n[Q2] |Large - Small|     = |184 - 1256| = 1072\n[Q3] |Train reggae drums - Val country vocals| = |83 - 17| = 66\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def find_long_silences(dataset_dict, sr=SR, threshold_sec=DURATION, top_db=TOP_DB):\n    \"\"\"\n    Input:\n        dataset_dict: The dictionary structure {genre: {stem: [paths...]}}\n    Output:\n        df: Pandas DataFrame containing details of all files with silence >= 5s\n    \"\"\"\n    records = []\n    # ------------------- write your code here -------------------------------\n\n    total_files = sum(len(paths) for genre_data in dataset_dict.values()\n                      for paths in genre_data.values())\n    print(f\"Analyzing {total_files} files for long silences...\")\n\n    pbar = tqdm(total=total_files, desc=\"Processing files\")\n\n    for genre, stems_dict in dataset_dict.items():\n        for stem_name, file_paths in stems_dict.items():\n            for file_path in file_paths:\n                try:\n                    # Load Audio\n                    y, _ = librosa.load(file_path, sr=sr, duration=None)\n                    total_duration = len(y) / sr\n\n                    # Find Non-Silent Intervals\n                    non_silent_intervals = librosa.effects.split(y, top_db=top_db)\n\n                    max_silence  = 0.0\n                    silence_type = []\n\n                    if len(non_silent_intervals) == 0:\n                        # CASE A: Fully silent\n                        max_silence  = total_duration\n                        silence_type = [\"start\", \"middle\", \"end\"]\n                    else:\n                        # CASE B: START silence\n                        start_silence = non_silent_intervals[0][0] / sr\n                        if start_silence > max_silence:\n                            max_silence  = start_silence\n                            silence_type = [\"start\"]\n\n                        # CASE C: END silence\n                        end_silence = (len(y) - non_silent_intervals[-1][1]) / sr\n                        if end_silence > max_silence:\n                            max_silence  = end_silence\n                            silence_type = [\"end\"]\n                        elif abs(end_silence - max_silence) < 0.01:\n                            if \"end\" not in silence_type:\n                                silence_type.append(\"end\")\n\n                        # CASE D: MIDDLE silence\n                        for i in range(len(non_silent_intervals) - 1):\n                            gap_start    = non_silent_intervals[i][1]\n                            gap_end      = non_silent_intervals[i + 1][0]\n                            gap_duration = (gap_end - gap_start) / sr\n\n                            if gap_duration > max_silence:\n                                max_silence  = gap_duration\n                                silence_type = [\"middle\"]\n                            elif abs(gap_duration - max_silence) < 0.01:\n                                if \"middle\" not in silence_type:\n                                    silence_type.append(\"middle\")\n\n                    # Store result if silence meets threshold\n                    if max_silence >= threshold_sec:\n                        records.append({\n                            \"Genre\":            genre,\n                            \"Stem\":             stem_name,\n                            \"Duration\":         round(total_duration, 2),\n                            \"Max_Silence_Sec\":  round(max_silence, 2),\n                            \"Silence_Location\": \", \".join(silence_type),\n                            \"File_Path\":        file_path\n                        })\n\n                except Exception as e:\n                    print(f\"Error processing {file_path}: {e}\")\n\n                pbar.update(1)\n\n    pbar.close()\n    #-------------------------------------------------------------------------\n    df = pd.DataFrame(records)\n    return df\n\n\n# --- EXECUTION ---\ndf_silence = find_long_silences(tr, threshold_sec=DURATION, top_db=TOP_DB)\n\n# --- RESULTS ANALYSIS ---\n# ------------------- write your code here -------------------------------\nprint(f\"\\nTotal files with silence >= {DURATION}s: {len(df_silence)}\")\nprint(df_silence.head())\n#-------------------------------------------------------------------------\n\n# Hint: Create a pivot Table: Count by Genre vs Stem\nif len(df_silence) > 0:\n    pivot_table = df_silence.pivot_table(index='Genre', columns='Stem', aggfunc='size', fill_value=0)\n    print(\"\\nPivot Table (Genre vs Stem):\")\n    print(pivot_table)\n\n# ---- Q4-Q9 Analysis ----\nprint()\nq4 = len(df_silence)\nprint(f\"[Q4] Total sound files with silence >= 5s          : {q4}\")\n\nq5 = len(df_silence[df_silence['Stem'] == 'vocals'])\nprint(f\"[Q5] Total Vocals tracks with silence >= 5s        : {q5}\")\n\nvocals_df = df_silence[df_silence['Stem'] == 'vocals']\nq6 = round(vocals_df['Max_Silence_Sec'].mean(), 2) if len(vocals_df) > 0 else 0.0\nprint(f\"[Q6] Average Silence Length in Vocals (secs)       : {q6}\")\n\njazz_drums = df_silence[(df_silence['Genre'] == 'jazz') & (df_silence['Stem'] == 'drums')]\nq7 = len(jazz_drums)\nprint(f\"[Q7] Jazz drums tracks with silence >= 5s          : {q7}\")\n\nq8 = len(jazz_drums[jazz_drums['Silence_Location'] == 'middle'])\nprint(f\"[Q8] Jazz drums (silence >= 5s, location=middle)   : {q8}\")\n\nq9 = len(jazz_drums[jazz_drums['Max_Silence_Sec'] >= 10.0])\nprint(f\"[Q9] Jazz drums (silence >= 5s, Max_Silence >= 10) : {q9}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T16:36:33.342235Z","iopub.execute_input":"2026-02-18T16:36:33.342547Z","iopub.status.idle":"2026-02-18T16:43:23.342842Z","shell.execute_reply.started":"2026-02-18T16:36:33.342510Z","shell.execute_reply":"2026-02-18T16:43:23.341573Z"}},"outputs":[{"name":"stdout","text":"Analyzing 3320 files for long silences...\n","output_type":"stream"},{"name":"stderr","text":"Processing files: 100%|██████████| 3320/3320 [06:49<00:00,  8.10it/s]","output_type":"stream"},{"name":"stdout","text":"\nTotal files with silence >= 5.0s: 680\n   Genre  Stem  Duration  Max_Silence_Sec Silence_Location  \\\n0  blues  bass     30.01             7.08           middle   \n1  blues  bass     30.01             8.68           middle   \n2  blues  bass     30.01             8.38            start   \n3  blues  bass     30.01             5.87           middle   \n4  blues  bass     30.01            21.80            start   \n\n                                           File_Path  \n0  /kaggle/input/jan-2026-dl-gen-ai-project/messy...  \n1  /kaggle/input/jan-2026-dl-gen-ai-project/messy...  \n2  /kaggle/input/jan-2026-dl-gen-ai-project/messy...  \n3  /kaggle/input/jan-2026-dl-gen-ai-project/messy...  \n4  /kaggle/input/jan-2026-dl-gen-ai-project/messy...  \n\nPivot Table (Genre vs Stem):\nStem       bass  drums  other  vocals\nGenre                                \nblues        17     22      5      43\nclassical    68     57      5      69\ncountry      16     16      2      16\ndisco         8      2      3      18\nhiphop       20      3     22       6\njazz         25     24      1      70\nmetal         6      2      1      40\npop          10      5      2       4\nreggae        5      4      7      12\nrock         10      7      1      26\n\n[Q4] Total sound files with silence >= 5s          : 680\n[Q5] Total Vocals tracks with silence >= 5s        : 304\n[Q6] Average Silence Length in Vocals (secs)       : 12.59\n[Q7] Jazz drums tracks with silence >= 5s          : 24\n[Q8] Jazz drums (silence >= 5s, location=middle)   : 16\n[Q9] Jazz drums (silence >= 5s, Max_Silence >= 10) : 7\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"stems_audio = []\ntry:\n    for key in STEM_KEYS:\n        # ------------------- write your code here -------------------------------\n        # Load audio (Duration 5.0s for speed/consistency)\n        stem_path = tr[GENRE_TO_TEST][key][SONG_INDEX]\n        y, sr_loaded = librosa.load(stem_path, sr=SR, duration=DURATION)\n        stems_audio.append(y)\n        print(f\"Loaded {key}: {stem_path}\")\n        print(f\"  Shape: {y.shape}, Sample rate: {sr_loaded}\")\n        #-------------------------------------------------------------------------\n\n    print(\"\\nAudio loaded successfully.\")\nexcept NameError:\n    print(\"ERROR: 'tr' dictionary not found. Please run build_dataset() first.\")\nexcept IndexError:\n    print(f\"ERROR: Song index {SONG_INDEX} out of range for genre {GENRE_TO_TEST}.\")\nexcept Exception as e:\n    print(f\"ERROR: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T16:43:23.344879Z","iopub.execute_input":"2026-02-18T16:43:23.346027Z","iopub.status.idle":"2026-02-18T16:43:23.400189Z","shell.execute_reply.started":"2026-02-18T16:43:23.345986Z","shell.execute_reply":"2026-02-18T16:43:23.399304Z"}},"outputs":[{"name":"stdout","text":"Loaded drums: /kaggle/input/jan-2026-dl-gen-ai-project/messy_mashup/genres_stems/rock/rock.00092/drums.wav\n  Shape: (110250,), Sample rate: 22050\nLoaded vocals: /kaggle/input/jan-2026-dl-gen-ai-project/messy_mashup/genres_stems/rock/rock.00092/vocals.wav\n  Shape: (110250,), Sample rate: 22050\nLoaded bass: /kaggle/input/jan-2026-dl-gen-ai-project/messy_mashup/genres_stems/rock/rock.00092/bass.wav\n  Shape: (110250,), Sample rate: 22050\nLoaded other: /kaggle/input/jan-2026-dl-gen-ai-project/messy_mashup/genres_stems/rock/rock.00092/other.wav\n  Shape: (110250,), Sample rate: 22050\n\nAudio loaded successfully.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# ------------------- write your code here -------------------------------\n# Stack them into a numpy array (Shape: 4 x Samples)\nstems_stack = np.array(stems_audio)\nprint(f\"Stems stack shape: {stems_stack.shape}\")\n\n# Mix the stems by summing them element-wise\nmix_raw = np.sum(stems_stack, axis=0)\nprint(f\"Mix raw shape: {mix_raw.shape}\")\nprint(f\"Mix raw length: {len(mix_raw)}\")\n\n# Calculate RMS Amplitude MANUALLY\nrms_val = np.sqrt(np.mean(mix_raw**2))\nprint(f\"RMS Amplitude: {rms_val:.4f}\")\n\n# Peak Normalization\nmax_val = np.max(np.abs(mix_raw))\nprint(f\"Max value (before normalization): {max_val:.4f}\")\n\nif max_val > 0:\n    mix_norm = mix_raw / max_val\nelse:\n    mix_norm = mix_raw\n\nprint(f\"Max value (after normalization): {np.max(np.abs(mix_norm)):.4f}\")\n\n# VALIDATION\nassert np.isclose(np.max(np.abs(mix_norm)), 1.0), \"Normalization failed.\"\nprint(\"\\nNormalization validation: PASSED\")\n#------------------------------------------------------------------------\n\n# ---- Q10-Q12 Analysis ----\nprint(f\"\\n[Q10] Length of mix sample          : {len(mix_raw)}\")\nprint(f\"[Q11] RMS Amplitude of mix sample   : {round(rms_val, 2)}\")\nprint(f\"[Q12] Max value of normalized sample: {round(float(np.max(np.abs(mix_norm))), 2)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T16:43:23.401372Z","iopub.execute_input":"2026-02-18T16:43:23.401722Z","iopub.status.idle":"2026-02-18T16:43:23.413252Z","shell.execute_reply.started":"2026-02-18T16:43:23.401692Z","shell.execute_reply":"2026-02-18T16:43:23.411886Z"}},"outputs":[{"name":"stdout","text":"Stems stack shape: (4, 110250)\nMix raw shape: (110250,)\nMix raw length: 110250\nRMS Amplitude: 0.1021\nMax value (before normalization): 0.5894\nMax value (after normalization): 1.0000\n\nNormalization validation: PASSED\n\n[Q10] Length of mix sample          : 110250\n[Q11] RMS Amplitude of mix sample   : 0.10000000149011612\n[Q12] Max value of normalized sample: 1.0\n","output_type":"stream"}],"execution_count":18}]}